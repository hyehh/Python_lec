{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f957e440",
   "metadata": {},
   "source": [
    "# 크롤링 \n",
    ": 전세계를 돌면서 정보를 축적 (주기적으로 돌림)\n",
    "# 스크래핑 \n",
    ": 스크랩하는 것이라고 생각 (필요한 것만 따로 정리)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1329ebde",
   "metadata": {},
   "source": [
    "## 데이터 다운로드 하기\n",
    ": 인터넷에서 지정된 파일을 내 PC에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9ee78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 가져오기\n",
    "# library\n",
    "# 요청하는 것\n",
    "import urllib.request\n",
    "\n",
    "# URL(어디서 가져올거니)과 저장경로(어디에 저장할거니) 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/test.png\"\n",
    "\n",
    "# 다운로드 하기 (이제 실행) : 이미지 뿐만 아니라 동영상도 이런식으로 가져올 수 있음\n",
    "urllib.request.urlretrieve(url, savename)\n",
    "print(\"저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87085843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 가져오기\n",
    "# library\n",
    "# 요청하는 것\n",
    "import urllib.request\n",
    "\n",
    "# URL(어디서 가져올거니)과 저장경로(어디에 저장할거니) 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/test1.png\"\n",
    "\n",
    "# 다운로드 하기 (또 다른 ver.)\n",
    "# 파일을 바로 다운로드 하지 않고 메모리로 가져와서 파일로 저장할 것임\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "# 파일로 저장하기 (그림은 binary file : w가 아닌 wb로 작성)\n",
    "with open(savename, \"wb\") as f:\n",
    "    f.write(mem)\n",
    "print(\"저장 되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77394267",
   "metadata": {},
   "source": [
    "# BeautifulSoup로 Scraping하기\n",
    ": 홈페이지에 있는 내용들 가져와서 태그 형태로 검색하기 쉬워서 많이 사용   \n",
    ": 간단하게 HTML과 XML에서 정보를 추출할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c3e2fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/parksunghun/opt/anaconda3/lib/python3.8/site-packages (4.9.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/parksunghun/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4) (2.2.1)\r\n"
     ]
    }
   ],
   "source": [
    "# 아나콘다에 자동으로 설치되어있지 않기에 수동으로 설치해줘야 함\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e80d5731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.Preview.ipynb                    09.for문.ipynb\r\n",
      "02.변수와 자료형.ipynb              10.함수.ipynb\r\n",
      "03.리스트.ipynb                     11.시뮬레이션.ipynb\r\n",
      "04.튜플.ipynb                       12.파일 입출력.ipynb\r\n",
      "05.Dictionary.ipynb                 13.분석 모듈 Preview.ipynb\r\n",
      "06.집합.ipynb                       14. 크롤링과 스크래핑.ipynb\r\n",
      "07.if문.ipynb                       \u001b[34mData\u001b[m\u001b[m\r\n",
      "08.while문.ipynb                    README.md\r\n"
     ]
    }
   ],
   "source": [
    "# ! 쓰면 외장 명령어 사용 가능!\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3c637a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html>\n",
      "<body>\n",
      "<h1>Header</h1>\n",
      "<p> Line 1을 서술 </p>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample (데이터를 가져왔다는 전제하에서 사용 : 이 부분은 연습해보는 것)\n",
    "html1 = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <h1>Header</h1>\n",
    "                <p> Line 1을 서술 </p>\n",
    "            </body>\n",
    "        </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석 (soup 으로 html1 을 파싱하는 것)\n",
    "# BeautifulSoup이 없으면 한 줄 한 줄 어제 배운 것처럼 한 줄씩 읽어와야 함\n",
    "soup = BeautifulSoup(html1, 'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cb15d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Header</h1>\n",
      "<p> Line 1을 서술 </p>\n",
      "---------\n",
      "h1= Header\n",
      "h1= Header\n",
      "p1=  Line 1을 서술 \n",
      "p1=  Line 1을 서술 \n"
     ]
    }
   ],
   "source": [
    "# 원하는 부분 추출하기\n",
    "# 태그 이용해 Header라는 글자 가져오기\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "print(h1)\n",
    "print(p1)\n",
    "print(\"---------\")\n",
    "\n",
    "# Text만 출력\n",
    "# h1.string : h1중에 string값만 가져오겠다!\n",
    "print(\"h1=\", h1.string)\n",
    "print(\"h1=\", h1.text)\n",
    "print(\"p1=\", p1.string)\n",
    "print(\"p1=\", p1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64640f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\">Header</h1>\n",
      "<p id=\"body\"> Line 1을 서술 </p>\n",
      "---------\n",
      "title = Header\n",
      "body =  Line 1을 서술 \n"
     ]
    }
   ],
   "source": [
    "# id로 요소를 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "html2 = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <h1 id=\"title\">Header</h1>\n",
    "                <p id=\"body\"> Line 1을 서술 </p>\n",
    "            </body>\n",
    "        </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(html2, 'html.parser')\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "# soup.find(id = 'title') : id가 title인 것을 찾아와리\n",
    "title = soup.find(id = 'title')\n",
    "body = soup.find(id = 'body')\n",
    "print(title)\n",
    "print(body)\n",
    "print(\"---------\")\n",
    "\n",
    "# text만 출력\n",
    "print(\"title =\", title.string)\n",
    "print(\"body =\", body.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb47bc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://www.naver.com\">naver</a>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 개의 요소 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "html3 = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <ul>\n",
    "                    <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "                    <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "                </ul>\n",
    "            </body>\n",
    "        </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(html3, 'html.parser')\n",
    "\n",
    "# a태그에 있는 건 다 가져오겠다!\n",
    "# find는 제일 처음에 있는 것만 가져옴!\n",
    "soup.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6016250c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://www.naver.com\">naver</a>,\n",
       " <a href=\"http://www.daum.net\">daum</a>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find_all은 다 가져옴! -> list로 가져옴\n",
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a52e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver\n",
      "daum\n",
      "---------\n",
      "http://www.naver.com\n",
      "http://www.daum.net\n",
      "---------\n",
      "naver >>> http://www.naver.com\n",
      "daum >>> http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all('a')\n",
    "\n",
    "# 링크 목록 출력\n",
    "for link in links:\n",
    "    # 태그 다 지우고 순수한 글자값만 가져옴\n",
    "    print(link.string)\n",
    "\n",
    "print(\"---------\")\n",
    "\n",
    "# 이름 필요 없고 링크만 필요한 경우\n",
    "for link in links:\n",
    "    # 아나콘다는 웹이기 때문에 클릭하면 바로 사이트로 이동함!\n",
    "    print(link.attrs['href'])\n",
    "    \n",
    "print(\"---------\")\n",
    "\n",
    "# 정리\n",
    "for link in links:\n",
    "    href = link.attrs['href']\n",
    "    text = link.string\n",
    "    print(text, \">>>\", href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09abf4ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "○ (강수) 16일(금) 오후에는 소나기가 내리는 곳이 있겠고, 19일(월) 오후에는 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 아침최저기온은 23~26도, 낮최고기온은 29~34도로 어제(12일, 아침최저기온 20~24도, 낮최고기온 30~33도)와 비슷하거나 조금 높겠습니다.<br />○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.<br />○ (주말전망) 17일(토)~18일(일)은 구름많겠고, 아침 기온은 23~24도, 낮 기온은 29~33도가 되겠습니다<br /><br />* 이번 예보기간 동안 내륙에는 낮최고기온이 33도 내외, 아침최저기온이 25도 내외로 오르는 곳이 많아 매우 무덥겠으니, 건강관리에 각별히 유의하기 바랍니다.<br />* 또한, 북태평양고기압 위치에 따른 강수 변동성이 크겠으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bs4.element.CData"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기상청 자료 활용하기 (최근 날씨 데이터 가져오기_\n",
    "# xml도 BeautifulSoup을 통해 가능! 아래 사이트 xml임\n",
    "# http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "\n",
    "# 데이터 가져오기\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# xml이지만 html.parser 사용 가능!\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "# print(soup)\n",
    "\n",
    "# <wf> : 요약정보 데이터 와 <title> 추출하기\n",
    "# 바로 find 뒤에 string 해도 괜찮음!\n",
    "title = soup.find('title').string\n",
    "print(title)\n",
    "wf = soup.find('wf').string\n",
    "print(wf)\n",
    "# bs4.element.CData : Character\n",
    "type(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "896a6970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "\n",
      "○ (강수) 16일(금) 오후에는 소나기가 내리는 곳이 있겠고, 19일(월) 오후에는 비가 내리겠습니다.\n",
      "○ (기온) 이번 예보기간 아침최저기온은 23~26도, 낮최고기온은 29~34도로 어제(12일, 아침최저기온 20~24도, 낮최고기온 30~33도)와 비슷하거나 조금 높겠습니다.\n",
      "○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.\n",
      "○ (주말전망) 17일(토)~18일(일)은 구름많겠고, 아침 기온은 23~24도, 낮 기온은 29~33도가 되겠습니다\n",
      "\n",
      "* 이번 예보기간 동안 내륙에는 낮최고기온이 33도 내외, 아침최저기온이 25도 내외로 오르는 곳이 많아 매우 무덥겠으니, 건강관리에 각별히 유의하기 바랍니다.\n",
      "* 또한, 북태평양고기압 위치에 따른 강수 변동성이 크겠으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "title = soup.find('title').string\n",
    "wf = soup.find('wf').string\n",
    "\n",
    "print(title)\n",
    "print()\n",
    "\n",
    "for i in wf.split('<br />'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02f079ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wf.split('<br />'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b2d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181b688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022027c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5674b19e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
